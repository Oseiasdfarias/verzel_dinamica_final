# ğŸ“š Rosana Desk - Sistema RAG com Llama 3.2

## DocumentaÃ§Ã£o Completa do Projeto

### ğŸ¯ VisÃ£o Geral

O **Rosana Desk** Ã© uma plataforma de automaÃ§Ã£o de atendimento que utiliza tÃ©cnicas de RAG (Retrieval-Augmented Generation) com o modelo Llama 3.2 local. O sistema combina busca vetorial (FAISS) com um modelo de linguagem para fornecer respostas precisas baseadas em uma base de conhecimento especÃ­fica.

---

## ğŸ“‹ Ãndice

1. [Arquitetura do Sistema](#arquitetura-do-sistema)
2. [Classes Principais](#classes-principais)
3. [RAG Chain - Pipeline de Processamento](#rag-chain---pipeline-de-processamento)
4. [Sistema de Templates](#sistema-de-templates)
5. [Fluxo de Dados Completo](#fluxo-de-dados-completo)
6. [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#instalaÃ§Ã£o-e-configuraÃ§Ã£o)
7. [Uso da API](#uso-da-api)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ—ï¸ Arquitetura do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND                              â”‚
â”‚                   (Cliente HTTP)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP Request
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Server                           â”‚
â”‚                    (porta 8000)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG CHAIN                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. Retriever (FAISS)                                 â”‚   â”‚
â”‚  â”‚    â†“                                                 â”‚   â”‚
â”‚  â”‚ 2. Prompt Template                                   â”‚   â”‚
â”‚  â”‚    â†“                                                 â”‚   â”‚
â”‚  â”‚ 3. LLM (Llama 3.2)                                   â”‚   â”‚
â”‚  â”‚    â†“                                                 â”‚   â”‚
â”‚  â”‚ 4. Output Parser                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Base de Conhecimento                       â”‚
â”‚                    (10 documentos)                           â”‚
â”‚                    â†•ï¸ Embeddings                              â”‚
â”‚                   FAISS Vector Store                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Classes Principais

### 1. **FastAPI**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
app = FastAPI(title="Rosana Desk - Local RAG (Llama 3.2)")
```

**DescriÃ§Ã£o:**
- Framework web moderno e assÃ­ncrono para Python
- Gera documentaÃ§Ã£o automÃ¡tica (Swagger UI em `/docs`)
- Suporta validaÃ§Ã£o de dados com Pydantic
- Alta performance comparÃ¡vel a Node.js

**FunÃ§Ãµes principais:**
- `@app.get()` - Define rotas GET
- `@app.post()` - Define rotas POST
- `app.add_middleware()` - Adiciona middlewares (CORS, etc)

**Uso no projeto:**
```python
# Cria servidor HTTP
# Define endpoints: /, /chat, /health
# Gerencia requisiÃ§Ãµes e respostas JSON
```

**Exemplo de requisiÃ§Ã£o:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Qual o horÃ¡rio de atendimento?"}'
```

---

### 2. **BaseModel (Pydantic)**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
class UserMessage(BaseModel):
    message: str
```

**DescriÃ§Ã£o:**
- Classe base do Pydantic para validaÃ§Ã£o de dados
- Converte automaticamente tipos de dados
- Gera erros detalhados quando dados estÃ£o incorretos

**ValidaÃ§Ãµes automÃ¡ticas:**
```python
# âœ… VÃ¡lido
{"message": "OlÃ¡"}

# âŒ InvÃ¡lido - falta campo obrigatÃ³rio
{}

# âŒ InvÃ¡lido - tipo errado
{"message": 123}  # Espera string
```

**Uso no projeto:**
```python
@app.post("/chat")
async def chat_endpoint(user_msg: UserMessage):
    # FastAPI valida automaticamente
    # Se passar, user_msg.message Ã© garantidamente string
    print(user_msg.message)
```

**Exemplo com mais campos:**
```python
class UserMessage(BaseModel):
    message: str
    user_id: Optional[int] = None
    timestamp: datetime = Field(default_factory=datetime.now)
```

---

### 3. **OllamaEmbeddings**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
embeddings = OllamaEmbeddings(model="llama3.2:latest")
```

**DescriÃ§Ã£o:**
- Converte texto em vetores numÃ©ricos (embeddings)
- Cada texto vira uma lista de ~4000 nÃºmeros
- Textos similares tÃªm vetores similares

**Como funciona:**
```python
# Texto original
texto = "O horÃ¡rio Ã© das 09h Ã s 18h"

# Embedding (simplificado)
vetor = [0.23, -0.45, 0.12, ..., 0.89]  # ~4096 dimensÃµes
```

**ComparaÃ§Ã£o de similaridade:**
```python
# Dois textos similares
texto1 = "Qual o horÃ¡rio?"
texto2 = "Que horas funciona?"

# Embeddings prÃ³ximos
vetor1 = [0.5, 0.3, 0.2, ...]
vetor2 = [0.52, 0.28, 0.21, ...]  # Muito similar!

# Textos diferentes
texto3 = "Como fazer bolo?"
vetor3 = [0.1, -0.8, 0.9, ...]  # Muito diferente!
```

**Uso no projeto:**
```python
# Converte toda base de conhecimento em vetores
knowledge_base = ["texto1", "texto2", ...]
vetores = embeddings.embed_documents(knowledge_base)

# Quando usuÃ¡rio pergunta, converte pergunta tambÃ©m
pergunta = "Qual horÃ¡rio?"
vetor_pergunta = embeddings.embed_query(pergunta)

# FAISS compara vetores e acha os mais similares
```

---

### 4. **ChatOllama**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
llm = ChatOllama(
    model="llama3.2:latest",
    temperature=0.7,
    num_predict=1024,
    top_k=40,
    top_p=0.9
)
```

**DescriÃ§Ã£o:**
- Interface para comunicar com o modelo Llama 3.2 local
- Gera texto baseado no prompt fornecido
- Roda localmente via Ollama

**ParÃ¢metros explicados:**

| ParÃ¢metro | Valor | O que faz |
|-----------|-------|-----------|
| `model` | llama3.2:latest | Modelo a ser usado |
| `temperature` | 0.7 | Criatividade (0=robÃ³tico, 1=criativo) |
| `num_predict` | 1024 | MÃ¡ximo de tokens na resposta |
| `top_k` | 40 | Considera top 40 palavras mais provÃ¡veis |
| `top_p` | 0.9 | Nucleus sampling (diversidade) |

**Exemplo de uso direto:**
```python
# Sem RAG
resposta = llm.invoke("OlÃ¡, como vai?")
print(resposta)
# Output: "OlÃ¡! Vou bem, obrigado. Como posso ajudar?"

# Com contexto
prompt_completo = """
Contexto: O horÃ¡rio Ã© 09h-18h
Pergunta: Qual o horÃ¡rio?
"""
resposta = llm.invoke(prompt_completo)
```

**Tipos de temperatura:**
```python
# temperature=0.0 (DeterminÃ­stico)
# Sempre: "O horÃ¡rio Ã© das 09h Ã s 18h"

# temperature=0.7 (Balanceado)
# Pode variar: "Funcionamos das 09h Ã s 18h"
#              "Atendemos das 09h atÃ© 18h"

# temperature=1.0 (Muito criativo)
# Pode inventar: "Estamos disponÃ­veis o dia todo!" âŒ
```

---

### 5. **FAISS (Facebook AI Similarity Search)**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
vector_store = FAISS.from_texts(knowledge_base, embedding=embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})
```

**DescriÃ§Ã£o:**
- Biblioteca otimizada para busca de vetores similares
- Extremamente rÃ¡pida mesmo com milhÃµes de documentos
- Usado por Facebook, Google, OpenAI

**Como funciona:**

```python
# 1. CriaÃ§Ã£o do Ã­ndice
knowledge_base = [
    "HorÃ¡rio: 09h-18h",      # Doc 0
    "WhatsApp usa PlugChat", # Doc 1
    "Backups Ã s 02h00",      # Doc 2
    ...
]

# FAISS converte em vetores e indexa
vector_store = FAISS.from_texts(knowledge_base, embeddings)

# 2. Busca por similaridade
pergunta = "Que horas funciona?"

# FAISS encontra os 3 docs mais similares
resultados = retriever.invoke(pergunta)
# Retorna: [
#   "HorÃ¡rio: 09h-18h",           # Score: 0.95
#   "Atendimento segunda-sexta",  # Score: 0.73
#   "Canal #emergencia no Slack"  # Score: 0.42
# ]
```

**VisualizaÃ§Ã£o da busca:**
```
Pergunta: "Como integrar WhatsApp?"
       â†“
   Embedding
       â†“
   [0.3, 0.7, -0.2, ...]
       â†“
   FAISS compara com todos vetores
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Doc 1: WhatsApp + PlugChat  âœ“   â”‚  95% similar
â”‚ Doc 5: API REST integraÃ§Ã£o  âœ“   â”‚  78% similar
â”‚ Doc 3: Webhooks             âœ“   â”‚  65% similar
â”‚ Doc 2: Backups              âœ—   â”‚  12% similar
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   Retorna top 3
```

**ParÃ¢metros do retriever:**
```python
retriever = vector_store.as_retriever(
    search_kwargs={
        "k": 3,              # Quantos docs retornar
        "fetch_k": 20,       # Quantos considerar antes de filtrar
        "score_threshold": 0.5  # Threshold de similaridade
    }
)
```

---

### 6. **ChatPromptTemplate**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
prompt = ChatPromptTemplate.from_template(template)
```

**DescriÃ§Ã£o:**
- Cria templates reutilizÃ¡veis com variÃ¡veis
- Formata prompts no padrÃ£o do modelo
- Suporta mÃºltiplas mensagens (system, user, assistant)

**Como funciona:**

```python
# Template com variÃ¡veis
template = """
System: VocÃª Ã© {nome}
Context: {contexto}
User: {pergunta}
"""

# LangChain substitui as variÃ¡veis
dados = {
    "nome": "Rosana Desk",
    "contexto": "HorÃ¡rio 09h-18h",
    "pergunta": "Qual o horÃ¡rio?"
}

prompt_formatado = prompt.invoke(dados)
# Result:
"""
System: VocÃª Ã© Rosana Desk
Context: HorÃ¡rio 09h-18h
User: Qual o horÃ¡rio?
"""
```

**Tipos de templates:**

```python
# 1. Template simples
ChatPromptTemplate.from_template("Responda: {question}")

# 2. Template com mÃºltiplas mensagens
ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© um assistente"),
    ("user", "{question}"),
])

# 3. Template com few-shot examples
ChatPromptTemplate.from_messages([
    ("system", "Traduza para inglÃªs"),
    ("user", "OlÃ¡"), ("assistant", "Hello"),
    ("user", "Tchau"), ("assistant", "Goodbye"),
    ("user", "{text}"),
])
```

---

### 7. **StrOutputParser**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
| StrOutputParser()
```

**DescriÃ§Ã£o:**
- Extrai apenas o texto da resposta do modelo
- Remove metadados e formataÃ§Ã£o extra
- Retorna string limpa

**Antes e depois:**

```python
# Output do LLM (objeto complexo)
{
    "content": "O horÃ¡rio Ã© das 09h Ã s 18h",
    "response_metadata": {
        "model": "llama3.2",
        "created_at": "2024-...",
        "done": true,
        "total_duration": 1234567
    },
    "id": "run-abc123",
    "usage_metadata": {...}
}

# Depois do StrOutputParser
"O horÃ¡rio Ã© das 09h Ã s 18h"
```

**Outros parsers disponÃ­veis:**

```python
# JSON Parser
from langchain_core.output_parsers import JsonOutputParser
# Output: {"resposta": "09h-18h", "tipo": "horÃ¡rio"}

# List Parser
from langchain_core.output_parsers import CommaSeparatedListParser
# Output: ["09h", "18h", "segunda", "sexta"]

# Pydantic Parser
from langchain_core.output_parsers import PydanticOutputParser
# Output: RespostaModel(horario="09h-18h", dias=["seg", "sex"])
```

---

### 8. **RunnablePassthrough**

**LocalizaÃ§Ã£o no cÃ³digo:**
```python
{"context": retriever, "question": RunnablePassthrough()}
```

**DescriÃ§Ã£o:**
- Passa dados adiante sem modificar
- Ãštil quando vocÃª quer preservar o input original
- Funciona como um "bypass" na chain

**Exemplo prÃ¡tico:**

```python
# Sem RunnablePassthrough (pergunta se perde)
chain = (
    retriever  # Retorna apenas contexto
    | prompt   # NÃ£o tem acesso Ã  pergunta original!
    | llm
)

# Com RunnablePassthrough (pergunta Ã© preservada)
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt   # Tem acesso tanto ao contexto quanto Ã  pergunta!
    | llm
)
```

**Casos de uso:**

```python
# 1. Preservar input original
{"original": RunnablePassthrough(), "processado": alguma_funcao}

# 2. Passar mÃºltiplos valores
{
    "query": RunnablePassthrough(),
    "timestamp": lambda x: datetime.now(),
    "user": lambda x: "sistema"
}

# 3. Debugging
chain = (
    RunnablePassthrough()  # NÃ£o modifica nada
    | lambda x: print(f"DEBUG: {x}") or x  # Log e passa adiante
    | proxima_etapa
)
```

---

## â›“ï¸ RAG Chain - Pipeline de Processamento

### Estrutura Completa

```python
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

### Operador Pipe (`|`) Explicado

O `|` no LangChain funciona como **composiÃ§Ã£o de funÃ§Ãµes**, similar ao pipe do Unix/Linux:

```bash
# Unix pipe
cat arquivo.txt | grep "erro" | wc -l

# LangChain pipe
input | funcao1 | funcao2 | funcao3
```

**Equivalente sem pipe:**
```python
def rag_chain_manual(question):
    # Etapa 1: Buscar contexto
    context = retriever.invoke(question)
    data = {"context": context, "question": question}
    
    # Etapa 2: Formatar prompt
    formatted_prompt = prompt.invoke(data)
    
    # Etapa 3: Gerar resposta
    llm_response = llm.invoke(formatted_prompt)
    
    # Etapa 4: Extrair texto
    final_response = StrOutputParser().invoke(llm_response)
    
    return final_response

# Com pipe (muito mais limpo!)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

### Fluxo Detalhado Passo a Passo

#### **ETAPA 1: DicionÃ¡rio Inicial**
```python
{"context": retriever, "question": RunnablePassthrough()}
```

**Input:** `"Qual o horÃ¡rio de atendimento?"`

**Processo:**
1. `retriever` busca no FAISS os 3 docs mais similares
2. `RunnablePassthrough()` mantÃ©m a pergunta original

**Output:**
```python
{
    "context": [
        "O horÃ¡rio de atendimento do suporte humano Ã© das 09h Ã s 18h, de segunda a sexta, exceto feriados.",
        "Para problemas crÃ­ticos, acionar o canal #emergencia no Slack da equipe.",
        "A empresa Verzel valoriza autonomia, comunicaÃ§Ã£o assÃ­ncrona e entrega contÃ­nua de valor."
    ],
    "question": "Qual o horÃ¡rio de atendimento?"
}
```

---

#### **ETAPA 2: Prompt Template**
```python
| prompt
```

**Input:** DicionÃ¡rio da Etapa 1

**Processo:**
Substitui `{context}` e `{question}` no template

**Output:**
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
VocÃª Ã© o Rosana Desk, assistente virtual especializado da empresa Verzel.

INSTRUÃ‡Ã•ES CRÃTICAS:
1. Use EXCLUSIVAMENTE as informaÃ§Ãµes do contexto fornecido
2. Se a resposta nÃ£o estiver no contexto, diga claramente que nÃ£o sabe
3. Seja prestativo e ofereÃ§a alternativas quando possÃ­vel
4. Mantenha linguagem profissional mas amigÃ¡vel
5. Formate respostas para melhor legibilidade

CONTEXTO DISPONÃVEL:
O horÃ¡rio de atendimento do suporte humano Ã© das 09h Ã s 18h, de segunda a sexta, exceto feriados.
Para problemas crÃ­ticos, acionar o canal #emergencia no Slack da equipe.
A empresa Verzel valoriza autonomia, comunicaÃ§Ã£o assÃ­ncrona e entrega contÃ­nua de valor.
<|eot_id|><|start_header_id|>user<|end_header_id|>
Qual o horÃ¡rio de atendimento?<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

---

#### **ETAPA 3: LLM (Llama 3.2)**
```python
| llm
```

**Input:** String formatada da Etapa 2

**Processo:**
1. Llama 3.2 lÃª o prompt completo
2. Identifica o contexto fornecido
3. Gera resposta baseada APENAS no contexto
4. Retorna objeto com resposta + metadados

**Output:**
```python
{
    "content": "O atendimento do suporte humano da Verzel funciona das 09h Ã s 18h, de segunda a sexta-feira, exceto em feriados. Para situaÃ§Ãµes crÃ­ticas fora desse horÃ¡rio, vocÃª pode acionar o canal #emergencia no Slack da equipe.",
    "response_metadata": {
        "model": "llama3.2:latest",
        "created_at": "2024-01-15T10:30:00Z",
        "done": true,
        "total_duration": 2847291234,
        "load_duration": 1234567,
        "prompt_eval_count": 156,
        "eval_count": 48,
        "eval_duration": 1234567890
    },
    "id": "run-abc123def456",
    "usage_metadata": {
        "input_tokens": 156,
        "output_tokens": 48,
        "total_tokens": 204
    }
}
```

---

#### **ETAPA 4: Output Parser**
```python
| StrOutputParser()
```

**Input:** Objeto complexo da Etapa 3

**Processo:**
Extrai apenas o campo `content`

**Output:**
```
"O atendimento do suporte humano da Verzel funciona das 09h Ã s 18h, de segunda a sexta-feira, exceto em feriados. Para situaÃ§Ãµes crÃ­ticas fora desse horÃ¡rio, vocÃª pode acionar o canal #emergencia no Slack da equipe."
```

---

### VisualizaÃ§Ã£o GrÃ¡fica do Fluxo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: "Qual o horÃ¡rio de atendimento?"                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ETAPA 1: DicionÃ¡rio + FAISS           â”‚
        â”‚                                       â”‚
        â”‚ retriever â†’ busca 3 docs similares    â”‚
        â”‚ RunnablePassthrough â†’ preserva query  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ {"context": [...], "question": "..."}â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ETAPA 2: Prompt Template              â”‚
        â”‚                                       â”‚
        â”‚ Substitui {context} e {question}      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ String formatada com tags especiais   â”‚
        â”‚ <|begin_of_text|>...                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ETAPA 3: LLM (Llama 3.2)              â”‚
        â”‚                                       â”‚
        â”‚ Gera resposta baseada no contexto     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Objeto complexo com metadata          â”‚
        â”‚ {content: "...", metadata: {...}}     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ETAPA 4: StrOutputParser              â”‚
        â”‚                                       â”‚
        â”‚ Extrai apenas o texto                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT: "O atendimento funciona das 09h Ã s 18h..."          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Modos de ExecuÃ§Ã£o da Chain

```python
# 1. SÃ­ncrono (bloqueia atÃ© terminar)
response = rag_chain.invoke("Qual o horÃ¡rio?")
print(response)

# 2. AssÃ­ncrono (nÃ£o bloqueia)
response = await rag_chain.ainvoke("Qual o horÃ¡rio?")

# 3. Streaming (resposta em tempo real)
for chunk in rag_chain.stream("Qual o horÃ¡rio?"):
    print(chunk, end="", flush=True)
# Output: "O atendimento func... iona das 09... h Ã s 18h..."

# 4. Batch (mÃºltiplas perguntas de uma vez)
perguntas = [
    "Qual o horÃ¡rio?",
    "Como integrar WhatsApp?",
    "Onde acessar o backup?"
]
respostas = rag_chain.batch(perguntas)
```

---

## ğŸ“ Sistema de Templates

### Anatomia do Template

```python
template = """
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
VocÃª Ã© o Rosana Desk, assistente virtual especializado da empresa Verzel.

INSTRUÃ‡Ã•ES CRÃTICAS:
1. Use EXCLUSIVAMENTE as informaÃ§Ãµes do contexto fornecido
2. Se a resposta nÃ£o estiver no contexto, diga claramente que nÃ£o sabe
3. Seja prestativo e ofereÃ§a alternativas quando possÃ­vel
4. Mantenha linguagem profissional mas amigÃ¡vel
5. Formate respostas para melhor legibilidade

CONTEXTO DISPONÃVEL:
{context}<|eot_id|><|start_header_id|>user<|end_header_id|>
{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
```

### Tags Especiais do Llama 3.2

| Tag | Significado | Uso |
|-----|-------------|-----|
| `<\|begin_of_text\|>` | InÃ­cio da conversa | Marca o comeÃ§o do prompt |
| `<\|start_header_id\|>system<\|end_header_id\|>` | Mensagem do sistema | InstruÃ§Ãµes e personalidade |
| `<\|start_header_id\|>user<\|end_header_id\|>` | Mensagem do usuÃ¡rio | Pergunta real |
| `<\|start_header_id\|>assistant<\|end_header_id\|>` | Resposta do assistente | Onde o modelo comeÃ§a a gerar |
| `<\|eot_id\|>` | End of Turn | Marca fim de cada turno |

**Por que essas tags existem?**

O Llama 3.2 foi treinado com essas tags especÃ­ficas. Ã‰ como ensinar um cachorro a sentar quando vocÃª fala "senta" - o modelo aprendeu que `<|start_header_id|>user<|end_header_id|>` significa "aqui vem a pergunta do usuÃ¡rio".

### SeÃ§Ãµes do Template

#### **1. System (Personalidade e Regras)**

```python
<|start_header_id|>system<|end_header_id|>
VocÃª Ã© o Rosana Desk, assistente virtual especializado da empresa Verzel.

INSTRUÃ‡Ã•ES CRÃTICAS:
1. Use EXCLUSIVAMENTE as informaÃ§Ãµes do contexto fornecido
2. Se a resposta nÃ£o estiver no contexto, diga claramente que nÃ£o sabe
...
```

**FunÃ§Ã£o:**
- Define QUEM Ã© o assistente
- Estabelece COMO ele deve se comportar
- Define REGRAS que deve seguir

**Analogia:**
Ã‰ como o treinamento que um atendente recebe antes de comeÃ§ar a trabalhar.

#### **2. Context (Conhecimento Relevante)**

```python
CONTEXTO DISPONÃVEL:
{context}
```

**FunÃ§Ã£o:**
- Injeta informaÃ§Ãµes relevantes recuperadas pelo FAISS
- O modelo DEVE usar essas informaÃ§Ãµes para responder
- Ã‰ a base do RAG (Retrieval-Augmented Generation)

**Exemplo de substituiÃ§Ã£o:**
```python
# Antes
{context}

# Depois (substituÃ­do pelo FAISS)
O horÃ¡rio de atendimento do suporte humano Ã© das 09h Ã s 18h, de segunda a sexta, exceto feriados.
Para problemas crÃ­ticos, acionar o canal #emergencia no Slack da equipe.
```

#### **3. User (Pergunta)**

```python
<|start_header_id|>user<|end_header_id|>
{question}<|eot_id|>
```

**FunÃ§Ã£o:**
- ContÃ©m a pergunta real do usuÃ¡rio
- SubstituÃ­da pela query recebida na API

**Exemplo:**
```python
# Antes
{question}

# Depois
Qual o horÃ¡rio de atendimento?
```

#### **4. Assistant (Resposta)**

```python
<|start_header_id|>assistant<|end_header_id|>
```

**FunÃ§Ã£o:**
- Marca onde o modelo deve comeÃ§ar a gerar a resposta
- NÃ£o tem conteÃºdo predefinido
- O Llama 3.2 continua a partir daqui

### Placeholders (VariÃ¡veis DinÃ¢micas)

**Sintaxe:**
```python
{nome_da_variavel}
```

**VariÃ¡veis no template atual:**
- `{context}` - SubstituÃ­do pelos docs do FAISS
- `{question}` - SubstituÃ­do pela pergunta do usuÃ¡rio

**Como adicionar mais variÃ¡veis:**

```python
template = """
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
VocÃª Ã© {assistant_name}, especialista em {specialty}.
Hoje Ã© {current_date}.
Seu idioma preferido Ã© {language}.

CONTEXTO:
{context}

<|start_header_id|>user<|end_header_id|>
{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

# Uso na chain
from datetime import datetime

rag_chain = (
    {
        "context": retriever,
        "question": RunnablePassthrough(),
        "assistant_name": lambda x: "Rosana Desk",
        "specialty": lambda x: "atendimento Verzel",
        "current_date": lambda x: datetime.now().strftime("%d/%m/%Y"),
        "language": lambda x: "portuguÃªs brasileiro"
    }
    | prompt
    | llm
    | StrOutputParser()
)
```

### Exemplo Completo de SubstituiÃ§Ã£o

#### **Template Original:**
```python
template = """
<|begin_of_text|><|start_header_